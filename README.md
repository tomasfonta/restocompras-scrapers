# restoCompras Scrapers

A modular, configurable web scraping framework for food suppliers in Argentina. Built to be easily extensible with support for both static (requests) and dynamic (Selenium) websites.

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [Environment Management](#-environment-management)
- [Project Structure](#-project-structure)
- [How It Works](#-how-it-works)
- [Adding a New Supplier](#-adding-a-new-supplier)
- [API Configuration](#-api-configuration)
- [Running All Suppliers](#-running-all-suppliers)
- [Troubleshooting](#-troubleshooting)
- [Architecture](#-architecture)


## ğŸš€ Quick Start

### Installation

```bash
# 1. Navigate to project directory
cd restocompras-scrapers

# 2. Install dependencies
pip3 install -r requirements.txt

# 3. Configure API settings
# Edit configs/api_config.dev.json and configs/api_config.prod.json
```

### Run a Scraper

```bash
# Development (localhost:8080) - Default
python3 main.py greenshop
python3 main.py greenshop --env dev

# Production (restocompras2.onrender.com)
python3 main.py greenshop --env prod

# List available suppliers
python3 main.py --list

# Custom directories
python3 main.py greenshop --config-dir ./configs --output-dir ./output
```

### What Happens

1. âœ… **Authenticates** with backend using supplier credentials
2. âœ… **Fetches supplier details** (ID and name) from API
3. âœ… **Cleans database** - Removes all existing items for this supplier
4. âœ… **Scrapes** product pages using configured strategy
5. âœ… **Extracts** and standardizes product data
6. âœ… **Deduplicates** products
7. âœ… **Looks up** product IDs from backend
8. âœ… **Posts** validated products to API
9. âœ… **Exports** results to Excel (`output/` directory)

**Output**: 
- `output/supplier_name_export_TIMESTAMP.xlsx`
- `logs/scraper_TIMESTAMP.log`

---

## ğŸŒ Environment Management

The scraper supports multiple environments (development and production) with easy switching via command-line arguments.

### Configuration Files

```
configs/
â”œâ”€â”€ api_config.dev.json       # Development environment (localhost:8080)
â”œâ”€â”€ api_config.prod.json      # Production environment (restocompras2.onrender.com)
â””â”€â”€ suppliers/
    â”œâ”€â”€ greenshop.json
    â””â”€â”€ ...
```

### Development Config (`api_config.dev.json`)
```json
{
  "base_url": "http://localhost:8080",
  "auth_token": "",
  "login_endpoint": "/login",
  "search_endpoint": "/api/products/search/best-match",
  "item_endpoint": "/api/item",
  "supplier_search_endpoint": "/api/suppliers/search",
  "supplier_delete_endpoint": "/api/ites/supplier/{supplier_id}",
  "timeout": 10
}
```

### Production Config (`api_config.prod.json`)
```json
{
  "base_url": "https://restocompras2.onrender.com",
  "auth_token": "",
  "login_endpoint": "/login",
  "search_endpoint": "/api/products/search/best-match",
  "item_endpoint": "/api/item",
  "supplier_search_endpoint": "/api/suppliers/search",
  "supplier_delete_endpoint": "/api/ites/supplier/{supplier_id}",
  "timeout": 30
}
```

### Usage

```bash
# Test locally
python3 main.py greenshop --env dev

# Run in production
python3 main.py greenshop --env prod

# Run all suppliers in production
for supplier in greenshop lacteos_granero tyna; do
    python3 main.py $supplier --env prod
done
```

**Key Features:**
- Separate configurations for dev/prod
- Different timeouts (10s for dev, 30s for prod)
- Isolated JWT tokens per environment
- Default to dev for safety
- Clear logging of which environment is active

---

## ğŸ“¦ Project Structure

```
scrapers/
â”œâ”€â”€ main.py                    # CLI entry point
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ api_config.dev.json   # Dev environment config
â”‚   â”œâ”€â”€ api_config.prod.json  # Prod environment config
â”‚   â””â”€â”€ suppliers/            # One JSON per supplier
â”‚       â”œâ”€â”€ greenshop.json
â”‚       â”œâ”€â”€ lacteos_granero.json
â”‚       â”œâ”€â”€ distribuidora_pop.json
â”‚       â”œâ”€â”€ tyna.json
â”‚       â””â”€â”€ labebidadetusfiestas.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ scraper_base.py   # Abstract base class
â”‚   â”‚   â”œâ”€â”€ api_client.py     # Backend API communication
â”‚   â”‚   â”œâ”€â”€ parser.py         # Data parsing utilities
â”‚   â”‚   â””â”€â”€ exporter.py       # Excel/JSON export
â”‚   â”œâ”€â”€ strategies/
â”‚   â”‚   â”œâ”€â”€ scraping_strategy.py      # Interface
â”‚   â”‚   â”œâ”€â”€ requests_strategy.py      # Static sites
â”‚   â”‚   â””â”€â”€ selenium_strategy.py      # Dynamic sites
â”‚   â”œâ”€â”€ suppliers/            # Supplier implementations
â”‚   â”‚   â”œâ”€â”€ greenshop.py
â”‚   â”‚   â”œâ”€â”€ lacteos_granero.py
â”‚   â”‚   â”œâ”€â”€ distribuidora_pop.py
â”‚   â”‚   â”œâ”€â”€ tyna.py
â”‚   â”‚   â””â”€â”€ labebidadetusfiestas.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config_loader.py  # JSON config loader
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py         # Logging setup
â”‚       â””â”€â”€ text_processing.py # Deduplication
â”œâ”€â”€ output/                   # Generated Excel files
â”œâ”€â”€ logs/                     # Execution logs
â””â”€â”€ legacy-scrappers/        # Old monolithic scripts (preserved)
```

---

## ğŸ”„ How It Works

### Complete Workflow

```
1. âœ… LOAD CONFIGURATIONS
   â”œâ”€ Load API config (environment-specific)
   â””â”€ Load supplier config (URLs, selectors, credentials)

2. âœ… AUTHENTICATE WITH BACKEND API
   â”œâ”€ Login with supplier credentials
   â”œâ”€ Retrieve JWT token
   â””â”€ Update auth_token in config file

3. âœ… FETCH SUPPLIER DETAILS
   â”œâ”€ Query backend API with supplier email
   â”œâ”€ Get supplier ID and name from backend
   â””â”€ Store supplier information

4. ğŸ†• CLEAN DATABASE
   â”œâ”€ DELETE /api/ites/supplier/{supplier_id}
   â”œâ”€ Remove all existing items for this supplier
   â””â”€ Prepare fresh database state
   
5. âœ… START SCRAPING PROCESS
   â”œâ”€ Fetch HTML from supplier website(s)
   â”œâ”€ Extract product data (name, price, unit, quantity, image)
   â”œâ”€ Parse and standardize data
   â””â”€ Deduplicate products

6. âœ… INTEGRATE WITH API
   â”œâ”€ For each product:
   â”‚  â”œâ”€ Search for product ID in backend
   â”‚  â””â”€ POST product to /api/item
   â””â”€ Track successfully posted products

7. âœ… EXPORT RESULTS
   â”œâ”€ Generate Excel file with product data
   â””â”€ Save to output/ directory

8. âœ… COMPLETE
   â””â”€ Display summary
```

### Data Flow Example

**Website HTML:**
```html
<div class="product">
  <h3>Tomate Cherry 500 gr</h3>
  <span class="price">$1.234,50</span>
  <img src="/images/tomate.jpg">
</div>
```

**Step 1 - Extract:**
```python
{'title': 'Tomate Cherry 500 gr', 'price': '$1.234,50', 'image': '/images/tomate.jpg'}
```

**Step 2 - Parse & Clean:**
```python
{'name': 'Tomate Cherry', 'quantity': 500, 'unit': 'G', 'price': 1234.50, 'image': 'https://site.com/images/tomate.jpg'}
```

**Step 3 - API Lookup:**
```
â†’ POST /api/products/search/best-match {"query": "Tomate Cherry 500 gr"}
â† Response: {"id": 123, "name": "Tomate Cherry"}
```

**Step 4 - Complete & Post:**
```python
{
  'name': 'Tomate Cherry', 'price': 1234.50, 'productId': 123,
  'unit': 'G', 'quantity': 500, 'supplierId': 1, 'brand': 'Green Shop',
  'image': 'https://site.com/images/tomate.jpg'
}
â†’ POST /api/item
â† Response: 201 Created
```

### Dual-Strategy Lookup

The API client uses a smart two-phase lookup:

1. **First attempt**: Search with full name + quantity + unit
   - Example: "Tomate Cherry 500 gr"
   
2. **Second attempt**: Search with only first 2 words
   - Example: "Tomate Cherry"
   
3. **Skip if not found**: Product logged as warning and skipped

This ensures maximum matching with backend products.

---

## â• Adding a New Supplier

### Step 1: Create Configuration

Create `configs/suppliers/supplier_name.json`:

**For Static Sites (requests):**
```json
{
  "scraping_strategy": "requests",
  "urls": ["https://example.com/products"],
  "selectors": {
    "product_list": ".product-item",
    "title": ".product-title", 
    "price": ".product-price",
    "image": ".product-image img"
  },
  "strategy_config": {
    "timeout": 15
  },
  "credentials": {
    "email": "supplier@restocompras.com",
    "password": "password"
  }
}
```

**For Dynamic Sites (selenium):**
```json
{
  "scraping_strategy": "selenium",
  "urls": ["https://example.com/products"],
  "selectors": {
    "product_list": "div.product-card",
    "title": "h2.title",
    "price": "span[class*='price']", 
    "image": "img.product-img"
  },
  "strategy_config": {
    "headless": true,
    "wait_time": 30,
    "scroll_attempts": 3
  },
  "credentials": {
    "email": "supplier@restocompras.com", 
    "password": "password"
  }
}
```

### Step 2: Create Scraper Class

Create `src/suppliers/supplier_name.py`:

```python
from typing import List, Dict, Any
from bs4 import BeautifulSoup
from urllib.parse import urljoin

from ..core.scraper_base import ScraperBase
from ..core.parser import DataParser
from ..strategies import RequestsStrategy  # or SeleniumStrategy


class SupplierNameScraper(ScraperBase):
    def __init__(self, config: Dict[str, Any], api_client: 'APIClient'):
        super().__init__(config, api_client)
        
        strategy_config = config.get('strategy_config', {})
        self.strategy = RequestsStrategy(strategy_config)
        self.selectors = config.get('selectors', {})
        self.parser = DataParser()
        self.base_url = config['urls'][0].split('/products')[0] if config.get('urls') else ''
    
    def get_urls(self) -> List[str]:
        return self.config.get('urls', [])
    
    def _fetch_html(self, url: str) -> str:
        return self.strategy.fetch_html(url)
    
    def extract_products(self, html_content: str, url: str) -> List[Dict[str, Any]]:
        soup = BeautifulSoup(html_content, 'html.parser')
        products = []
        
        product_items = soup.select(self.selectors['product_list'])
        
        for item in product_items:
            try:
                # Extract data
                title_element = item.select_one(self.selectors['title'])
                price_element = item.select_one(self.selectors['price'])
                image_element = item.select_one(self.selectors['image'])
                
                if not title_element or not price_element:
                    continue
                
                # Parse data
                title = title_element.get_text(strip=True)
                price_text = price_element.get_text(strip=True)
                
                name, quantity, unit = self.parser.parse_product_title(title)
                price, _ = self.parser.clean_price(price_text)
                
                if price <= 0:
                    continue
                
                # Handle image URL
                image_url = ''
                if image_element:
                    image_url = image_element.get('src', '')
                    if image_url and not image_url.startswith('http'):
                        image_url = urljoin(self.base_url, image_url)
                
                # Build product
                product = {
                    'name': name,
                    'price': price,
                    'unit': unit,
                    'quantity': quantity,
                    'supplierId': self.config.get('supplier_id', 0),
                    'brand': self.config.get('supplier_name', ''),
                    'description': name,
                    'image': image_url
                }
                
                products.append(product)
                
            except Exception as e:
                self.logger.error(f"Error extracting product: {e}")
                continue
        
        return products
    
    def __del__(self):
        if hasattr(self, 'strategy'):
            self.strategy.close()
```

### Step 3: Register Scraper

Add to `main.py`:

```python
from src.suppliers import (
    GreenShopScraper,
    LacteosGraneroScraper, 
    SupplierNameScraper  # Add import
)

SCRAPER_REGISTRY = {
    'greenshop': GreenShopScraper,
    'lacteos_granero': LacteosGraneroScraper,
    'supplier_name': SupplierNameScraper,  # Add here
}
```

### Step 4: Test

1. **Create supplier in backend first** with matching email/password
2. **Test the scraper**:
   ```bash
   python3 main.py supplier_name --env dev
   ```
3. **Check output**: Excel file in `output/` and logs in `logs/`

### Finding Selectors

Use browser DevTools (F12):
1. Right-click on product â†’ Inspect
2. Find unique CSS selectors for:
   - Product container
   - Product title/name
   - Product price 
   - Product image
3. Test in console: `document.querySelectorAll('.your-selector')`

### Strategy Choice

- **Use `requests`**: If product data is visible in page source (static HTML)
- **Use `selenium`**: If products load via JavaScript (dynamic content)

**Time estimate**: 30-60 minutes for simple sites

---

## ğŸ”Œ API Configuration

### Centralized Endpoints

All API endpoints are configured in environment-specific JSON files for easy management.

### Configuration Properties

| Property | Description | Example |
|----------|-------------|---------|
| `base_url` | Backend server URL | `http://localhost:8080` |
| `auth_token` | JWT token (auto-updated) | `eyJhbGciOiJIUzUxMiJ9...` |
| `login_endpoint` | Authentication endpoint | `/login` |
| `search_endpoint` | Product search endpoint | `/api/products/search/best-match` |
| `item_endpoint` | Product posting endpoint | `/api/item` |
| `supplier_search_endpoint` | Supplier lookup endpoint | `/api/suppliers/search` |
| `supplier_delete_endpoint` | Delete supplier items | `/api/ites/supplier/{supplier_id}` |
| `timeout` | Request timeout (seconds) | `10` (dev), `30` (prod) |

### Dynamic Placeholders

Some endpoints support placeholders replaced at runtime:

```json
{
  "supplier_delete_endpoint": "/api/ites/supplier/{supplier_id}"
}
```

**Runtime**: `/api/ites/supplier/10` (when supplier_id=10)

### Authentication

All API requests automatically include the Authorization header:

```python
{
    'Authorization': 'Bearer eyJhbGciOiJIUzUxMiJ9...',
    'Content-Type': 'application/json'
}
```

**Token Lifecycle:**
1. Load token from config
2. Login with credentials â†’ Get new token
3. Update token in config and all requests
4. Token persists for subsequent requests

---

## ğŸš€ Running All Suppliers

### Run All Suppliers Scripts

**Python Script (Cross-platform):**
```bash
# Development (default)
python3 run_all_suppliers.py
python3 run_all_suppliers.py --env dev

# Production
python3 run_all_suppliers.py --env prod
```

**Bash Script (Unix/Linux/macOS):**
```bash
# Development (default)
./run_all_suppliers.sh
./run_all_suppliers.sh --env dev

# Production
./run_all_suppliers.sh --env prod
```

**What it does:**
- Runs all configured suppliers sequentially
- Shows real-time progress with color-coded output
- Displays environment being used
- Generates summary report with success/failure counts
- Exit code 0 if all succeed, 1 if any fail

### Manual Iteration

```bash
# Development
for supplier in greenshop lacteos_granero distribuidora_pop tyna labebidadetusfiestas; do
    python3 main.py "$supplier" --env dev
done

# Production
for supplier in greenshop lacteos_granero distribuidora_pop tyna labebidadetusfiestas; do
    python3 main.py "$supplier" --env prod
done
```

### Available Suppliers

**Web Scrapers (HTML-based):**
- âœ… **greenshop** - Green Shop (Requests strategy)
- âœ… **lacteos_granero** - LÃ¡cteos Granero (Selenium strategy)
- âœ… **distribuidora_pop** - Distribuidora Pop (Requests strategy)
- âœ… **tyna** - Tyna (Requests strategy)
- âœ… **labebidadetusfiestas** - La Bebida de Tus Fiestas (Requests strategy)
- âœ… **piala** - Piala (Requests strategy)
- âœ… **distribuidora_demarchi** - Distribuidora De Marchi (Requests strategy)
- âœ… **laduvalina** - La Duvalina (Requests strategy)

**File-Based Scrapers:**
- âœ… **irlanda** - Irlanda (PDF price list strategy)
- âœ… **el_chanar_carnes** - El ChaÃ±ar Carnes (Excel price list strategy)

### Scheduling Automated Runs

**Using Cron (Linux/macOS):**
```bash
crontab -e
```

Add entry to run all suppliers daily at 2 AM in production:
```bash
0 2 * * * cd /path/to/scrapers && /usr/bin/python3 run_all_suppliers.py --env prod >> cron.log 2>&1
```

Or run individual supplier:
```bash
0 2 * * * cd /path/to/scrapers && /usr/bin/python3 main.py greenshop --env prod >> cron.log 2>&1
```

---

## ï¿½ Testing File-Based Providers

The framework supports file-based scrapers for PDF and Excel price lists, in addition to web scraping.

### PDF Price Lists (Irlanda)

**Use Case**: Suppliers that provide price lists as PDF documents.

#### Setup
1. Place PDF file in `input/` directory:
   ```bash
   cp LISTAS_IRLANDA.pdf input/
   ```

2. Configuration (`configs/suppliers/irlanda.json`):
   ```json
   {
     "supplier_id": 5,
     "supplier_name": "Irlanda",
     "scraping_strategy": "pdf",
     "credentials": {
       "name": "irlanda@restocompras.com",
       "password": "password"
     },
     "file_config": {
       "filename": "LISTAS_IRLANDA.pdf",
       "input_dir": "input",
       "strategy_type": "pdf"
     },
     "pdf_config": {
       "text_mode": true,
       "table_settings": {
         "vertical_strategy": "text",
         "horizontal_strategy": "text"
       }
     }
   }
   ```

#### Run PDF Scraper
```bash
# Development
python3 main.py irlanda --env dev

# Production
python3 main.py irlanda --env prod
```

#### Expected Output
```
INFO - Extracted 834 raw records from PDF
INFO - Processing 834 raw records
INFO - Successfully extracted 625 products from PDF
INFO - âœ… Successfully posted 'SODA SIFON SOCIAL' (Product ID: 113, Supplier ID: 5)
INFO - Export file: output/irlanda_export_20251102_111344.xlsx
```

#### PDF Format Support
- **Text mode**: Line-by-line extraction with regex patterns
  - Format: `CODE DESCRIPTION........ PRICE`
  - Example: `0101137 SODA SIFON SOCIAL 2L.................. 5700.00`
- **Table mode**: Structured table extraction using pdfplumber
- Handles multi-page PDFs automatically

### Excel Price Lists (El ChaÃ±ar Carnes)

**Use Case**: Suppliers that provide price lists as Excel spreadsheets.

#### Setup
1. Place Excel file in `input/` directory:
   ```bash
   cp "LISTA DE PRECIOS WHATSAPP Y OTROS.xlsx" input/
   ```

2. Configuration (`configs/suppliers/el_chanar_carnes.json`):
   ```json
   {
     "supplier_id": 6,
     "supplier_name": "El ChaÃ±ar carnes",
     "scraping_strategy": "excel",
     "credentials": {
       "name": "elchanar@restocompras.com",
       "password": "password"
     },
     "file_config": {
       "filename": "LISTA DE PRECIOS WHATSAPP Y OTROS.xlsx",
       "input_dir": "input",
       "strategy_type": "excel"
     },
     "excel_config": {
       "sheet_name": 0,
       "header_row": null,
       "skip_rows": 3,
       "use_pandas": true
     },
     "column_mapping": {
       "name_columns": [1, 5],
       "price_columns": [2, 6],
       "process_mode": "paired"
     }
   }
   ```

#### Run Excel Scraper
```bash
# Development
python3 main.py el_chanar_carnes --env dev

# Production
python3 main.py el_chanar_carnes --env prod
```

#### Expected Output
```
INFO - Extracted 150 raw records from Excel
INFO - Processing 150 raw records in paired mode
INFO - Successfully extracted 75 products from Excel
INFO - âœ… Successfully posted 'Bife s/lomo' (Product ID: 1, Supplier ID: 6)
INFO - Export file: output/el_chaÃ±ar_carnes_export_20251102_111358.xlsx
```

#### Excel Layout Support
**Paired Columns Mode** (Name1|Price1|Name2|Price2):
```
| Product A | $100 | Product C | $300 |
| Product B | $200 | Product D | $400 |
```

**Single Column Mode** (Name|Price):
```
| Product A | $100 |
| Product B | $200 |
```

Configure via `process_mode`: `"paired"` or `"single"`

### Adding New File-Based Suppliers

#### For PDF Price Lists

1. **Create config** (`configs/suppliers/supplier_name.json`):
   ```json
   {
     "supplier_id": 7,
     "supplier_name": "Supplier Name",
     "scraping_strategy": "pdf",
     "credentials": {
       "name": "supplier@restocompras.com",
       "password": "password"
     },
     "file_config": {
       "filename": "pricelist.pdf",
       "input_dir": "input",
       "strategy_type": "pdf"
     },
     "pdf_config": {
       "text_mode": true
     }
   }
   ```

2. **Place PDF file**: `input/pricelist.pdf`

3. **Test**:
   ```bash
   python3 main.py supplier_name --env dev
   ```

#### For Excel Price Lists

1. **Create config** (`configs/suppliers/supplier_name.json`):
   ```json
   {
     "supplier_id": 8,
     "supplier_name": "Supplier Name",
     "scraping_strategy": "excel",
     "credentials": {
       "name": "supplier@restocompras.com",
       "password": "password"
     },
     "file_config": {
       "filename": "pricelist.xlsx",
       "input_dir": "input",
       "strategy_type": "excel"
     },
     "excel_config": {
       "sheet_name": 0,
       "skip_rows": 0,
       "use_pandas": true
     },
     "column_mapping": {
       "name_columns": [0],
       "price_columns": [1],
       "process_mode": "single"
     }
   }
   ```

2. **Place Excel file**: `input/pricelist.xlsx`

3. **Test**:
   ```bash
   python3 main.py supplier_name --env dev
   ```

### File-Based Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FileStrategy   â”‚  Abstract base for file-based scraping
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDF    â”‚ â”‚    Excel     â”‚
â”‚Strategy â”‚ â”‚  Strategy    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚              â”‚
    â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Irlanda â”‚ â”‚ El ChaÃ±ar Carnes â”‚
â”‚ Scraper â”‚ â”‚    Scraper       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dependencies for File Processing

```bash
# PDF processing
pip install pdfplumber>=0.10.0

# Excel processing (already included)
pip install pandas>=2.1.0
pip install openpyxl>=3.1.0
```

### Troubleshooting File-Based Scrapers

| Issue | Solution |
|-------|----------|
| **File not found** | Ensure file is in `input/` directory with exact filename from config |
| **Empty PDF extraction** | Try switching `text_mode` between `true` and `false` |
| **Excel column errors** | Verify column indices in `name_columns` and `price_columns` (0-based) |
| **No products extracted** | Check `skip_rows` setting, inspect file structure manually |
| **Price parsing errors** | Update `price_format` in config (decimal/thousands separators) |

---

## ï¿½ğŸ”§ Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| **Authentication failed** | Check credentials in supplier config, verify supplier exists in backend |
| **No products found** | Update selectors in config, check website structure with DevTools |
| **ChromeDriver not found** | Install: `brew install chromedriver` (macOS) or download manually |
| **Products not posted to API** | Ensure products exist in backend database, check logs for product ID lookup failures |
| **401 Unauthorized** | Token expired - Run scraper again to get fresh token |
| **Delete failed** | Check endpoint URL, verify supplier ID exists, review logs |
| **Wrong environment** | Check CLI argument: `--env dev` or `--env prod` |
| **Timeout errors** | Increase timeout in config (especially for production) |

### Debug Mode

Check logs for detailed information:
```bash
# View latest log
ls -t logs/scraper_*.log | head -1 | xargs cat

# Search for errors
grep "ERROR" logs/scraper_*.log

# Search for specific product
grep "Tomate Cherry" logs/scraper_*.log
```

### Verify Environment

```bash
python3 main.py greenshop --env prod 2>&1 | grep "Environment:"
# Should show: INFO - Environment: PROD
```

---

## ğŸ—ï¸ Architecture

### Design Patterns

1. **Strategy Pattern**: Different scraping approaches (Requests vs Selenium)
2. **Template Method**: `ScraperBase` defines workflow, suppliers implement specifics
3. **Configuration-Driven**: All settings in JSON files
4. **Backend Integration**: Dynamic data fetching from API

### Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    main.py      â”‚  CLI entry point
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ConfigLoader   â”‚  Environment-aware config loading
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   APIClient     â”‚  Backend communication + Auth
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ScraperBase    â”‚  Base workflow (Template Method)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Supplier Scrapers              â”‚
â”‚  - Strategy selection           â”‚
â”‚  - Selector configuration       â”‚
â”‚  - Product extraction logic     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Requests â”‚ â”‚ Selenium â”‚  Scraping strategies
â”‚Strategy â”‚ â”‚ Strategy â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Processing Pipeline

```
Website HTML
    â†“
Extract raw data (BeautifulSoup + CSS selectors)
    â†“
Parse titles ("Tomate 500 gr" â†’ name, quantity, unit)
    â†“
Clean prices ("$1.234,50" â†’ 1234.50)
    â†“
Standardize units (grâ†’G, kgâ†’KG, unâ†’UNIT)
    â†“
Deduplicate by (name, unit, quantity)
    â†“
API Lookup (dual-strategy: full name â†’ short name)
    â†“
POST to backend (/api/item)
    â†“
Export to Excel
```

### Logging System

```
DEBUG   â†’ Detailed scraping info (file only)
INFO    â†’ Progress updates (console + file)
WARNING â†’ Skipped products, fallback strategies
ERROR   â†’ Failed operations, exceptions
CRITICALâ†’ System failures
```

**Log Format:**
- `â†’` Outgoing requests
- `â†` Incoming responses
- `âœ“` Success indicators
- `âœ—` Error indicators
- `âš ` Warning indicators

---

## ğŸ“ Summary

**restoCompras Scrapers** is a production-ready framework that:

âœ… **Automates** supplier data collection  
âœ… **Integrates** seamlessly with backend API  
âœ… **Scales** easily - add suppliers in minutes  
âœ… **Maintains** data quality through validation and deduplication  
âœ… **Supports** multiple environments (dev/prod)  
âœ… **Cleans** database before each scrape for fresh data  
âœ… **Logs** comprehensive details for debugging  
âœ… **Exports** standardized Excel reports  

**Key Philosophy**: Backend is the source of truth for business data; configs only contain technical scraping details.

---

## ğŸ“„ License

MIT
